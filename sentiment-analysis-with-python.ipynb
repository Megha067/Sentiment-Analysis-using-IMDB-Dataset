{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.feature_extraction.text import CountVectorizer\ncount=CountVectorizer()\ndata=pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T03:55:57.701295Z","iopub.execute_input":"2022-05-03T03:55:57.702530Z","iopub.status.idle":"2022-05-03T03:56:00.237620Z","shell.execute_reply.started":"2022-05-03T03:55:57.702389Z","shell.execute_reply":"2022-05-03T03:56:00.236579Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**let’s visualize the distribution of the data**","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=(5,5))\ncolors=[\"skyblue\",'pink']\npos=data[data['label']==1]\nneg=data[data['label']==0]\nck=[pos['label'].count(),neg['label'].count()]\nlegpie=plt.pie(ck,labels=[\"Positive\",\"Negative\"],\n                 autopct ='%1.1f%%', \n                 shadow = True,\n                 colors = colors,\n                 startangle = 45,\n                 explode=(0, 0.1))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:02:27.070567Z","iopub.execute_input":"2022-05-03T04:02:27.070868Z","iopub.status.idle":"2022-05-03T04:02:27.246505Z","shell.execute_reply.started":"2022-05-03T04:02:27.070832Z","shell.execute_reply":"2022-05-03T04:02:27.245746Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Then we will import RE, that is, the regular expression operation, we use this library to remove html tags like ‘<a>’ or. So whenever we come across these tags, we replace them with an empty string. Then we will also modify the emojis/emoticons which can be smileys :), a sad face: (or even an upset face: /. We will change the emojis towards the end to get a clean set of text:","metadata":{}},{"cell_type":"code","source":"import re\ndef preprocessor(text):\n             text=re.sub('<[^>]*>','',text)\n             emojis=re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n             text=re.sub('[\\W]+',' ',text.lower()) +\\\n                ' '.join(emojis).replace('-','')\n             return text   \ndata['text']=data['text'].apply(preprocessor)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:03:09.741241Z","iopub.execute_input":"2022-05-03T04:03:09.741524Z","iopub.status.idle":"2022-05-03T04:03:17.398701Z","shell.execute_reply.started":"2022-05-03T04:03:09.741496Z","shell.execute_reply":"2022-05-03T04:03:17.397878Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Now, I’ll be using nltk’s PorterStemmer to simplify the data and remove unnecessary complexities in our text data:","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nporter=PorterStemmer()\ndef tokenizer(text):\n        return text.split()\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:04:16.417585Z","iopub.execute_input":"2022-05-03T04:04:16.418017Z","iopub.status.idle":"2022-05-03T04:04:16.997488Z","shell.execute_reply.started":"2022-05-03T04:04:16.417986Z","shell.execute_reply":"2022-05-03T04:04:16.996570Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing Negative and Positive Words\nTo visualzie the negative and postive words using a wordcloud, I will first remove the stopwords:**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:05:27.638232Z","iopub.execute_input":"2022-05-03T04:05:27.638487Z","iopub.status.idle":"2022-05-03T04:05:27.789682Z","shell.execute_reply.started":"2022-05-03T04:05:27.638460Z","shell.execute_reply":"2022-05-03T04:05:27.788810Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\npositivedata = data[ data['label'] == 1]\npositivedata =positivedata['text']\nnegdata = data[data['label'] == 0]\nnegdata= negdata['text']\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:05:44.345467Z","iopub.execute_input":"2022-05-03T04:05:44.345789Z","iopub.status.idle":"2022-05-03T04:05:44.387717Z","shell.execute_reply.started":"2022-05-03T04:05:44.345733Z","shell.execute_reply":"2022-05-03T04:05:44.386775Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def wordcloud_draw(data, color = 'white'):\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()\n                              if(word!='movie' and word!='film')\n                            ])\n    wordcloud = WordCloud(stopwords=stop,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(10, 7))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:05:59.181810Z","iopub.execute_input":"2022-05-03T04:05:59.182091Z","iopub.status.idle":"2022-05-03T04:05:59.189206Z","shell.execute_reply.started":"2022-05-03T04:05:59.182058Z","shell.execute_reply":"2022-05-03T04:05:59.188493Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(\"Positive words are as follows\")\nwordcloud_draw(positivedata,'white')\nprint(\"Negative words are as follows\")\nwordcloud_draw(negdata)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:06:12.896824Z","iopub.execute_input":"2022-05-03T04:06:12.897285Z","iopub.status.idle":"2022-05-03T04:07:17.763153Z","shell.execute_reply.started":"2022-05-03T04:06:12.897233Z","shell.execute_reply":"2022-05-03T04:07:17.762326Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The positive words that are highlighted are love, excellent, perfect, good, beautiful, kind, excellent and The negative words that are highlighted are: horrible, wasteful, problem, stupid, horrible, bad, poor.\n\nNow I will use the TF-IDF Vertorizer to convert the raw documents into feature matrix which is very important to train a Machine Learning model:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,tokenizer=tokenizer_porter,use_idf=True,norm='l2',smooth_idf=True)\ny=data.label.values\nx=tfidf.fit_transform(data.text)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:13:57.955333Z","iopub.execute_input":"2022-05-03T04:13:57.955646Z","iopub.status.idle":"2022-05-03T04:17:51.646174Z","shell.execute_reply.started":"2022-05-03T04:13:57.955612Z","shell.execute_reply":"2022-05-03T04:17:51.645220Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Training Machine Learning Model for Sentiment Analysis**\n\n**Now to train a machine learning model I will split the data into 50 percent training and 50 percent test sets:**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.5,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:18:04.059321Z","iopub.execute_input":"2022-05-03T04:18:04.059752Z","iopub.status.idle":"2022-05-03T04:18:04.133617Z","shell.execute_reply.started":"2022-05-03T04:18:04.059709Z","shell.execute_reply":"2022-05-03T04:18:04.132868Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Now let’s train a machine learning model for the task of sentiment analysis by using the Logistic Regression model:**","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegressionCV\nclf=LogisticRegressionCV(cv=6,scoring='accuracy',random_state=0,n_jobs=-1,verbose=3,max_iter=500).fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nfrom sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T04:18:07.891466Z","iopub.execute_input":"2022-05-03T04:18:07.891759Z","iopub.status.idle":"2022-05-03T04:19:36.901395Z","shell.execute_reply.started":"2022-05-03T04:18:07.891728Z","shell.execute_reply":"2022-05-03T04:19:36.900138Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Accuracy: 0.89045**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}